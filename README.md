# ğŸ§  RAG Demo com PDF + Cohere + Pinecone + Groq

---

## ğŸ“Œ O que Ã© isso?

Este projeto Ã© uma demonstraÃ§Ã£o prÃ¡tica de **RAG (Retrieval-Augmented Generation)** usando:

- **PDFs como fonte de dados**
- **Embeddings com Cohere**
- **Armazenamento vetorial com Pinecone**
- **LLM da Groq para responder perguntas**

> ğŸ“š VocÃª envia um PDF â†’ Ele Ã© dividido em partes â†’ Embeddings sÃ£o gerados â†’ Consultas sÃ£o feitas com inteligÃªncia contextual.

---

## ğŸ’  Tecnologias Usadas

- ğŸ§± **LangChain**
- ğŸ§  **Cohere API (embeddings)**
- ğŸ“¦ **Pinecone (vector store)**
- ğŸ¤– **Groq (LLM via LangChain)**
- ğŸŒ¿ **dotenv (variÃ¡veis de ambiente)**
- ğŸ **Python 3.10+**

---

## ğŸ“‚ Estrutura do Projeto

```
ğŸ“ raiz/
ğŸ‘¥â”‚
ğŸ“‚ data/
â”‚   â””â”€â”€ 2210.03629v3.pdf        # PDF usado para a demo
â”œâ”€â”€ main.py                     # Script principal
â”œâ”€â”€ .env                        # Suas chaves (NÃƒO suba isso pro Git)
â””â”€â”€ requirements.txt            # DependÃªncias
```

---

## âš™ï¸ InstalaÃ§Ã£o Passo a Passo

### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/seu-usuario/seu-repo.git
cd seu-repo
```

### 2. Crie um ambiente virtual

```bash
python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate
```

### 3. Instale as dependÃªncias

```bash
pip install -r requirements.txt
```

### 4. Crie o arquivo `.env`

```
COHERE_API_KEY=your-cohere-api-key
PINECONE_API_KEY=your-pinecone-api-key
GROQ_API_KEY=your-groq-api-key
```

---

## ğŸš€ Como Rodar

1. Adicione um PDF na pasta `data/`.
2. Altere o caminho do arquivo no script, se necessÃ¡rio:

```python
PATH_FILE = "data\\2210.03629v3.pdf"
```

3. Execute:

```bash
python main.py
```

---

## ğŸ’¡ O que acontece no script?

```text
1. Carrega o PDF
2. Divide em pedaÃ§os (chunks)
3. Gera embeddings via Cohere
4. Salva os vetores no Pinecone
5. Usa Groq para responder com contexto
```

---

## ğŸ§ª Exemplo de ExecuÃ§Ã£o

```bash
Carregando os documentos ...
splitting the documments into small chunks ...

total chunks: 12

Resposta:
A large language model (LLM) is an advanced AI model trained on vast text data...
```

---

## ğŸ› ï¸ CustomizaÃ§Ãµes

Quer mudar a pergunta?

```python
res = qa.invoke({"input": "what is a llm?"})
```

Quer trocar o PDF? Ã‰ sÃ³ substituir o arquivo na pasta `data/`.

---

## âš ï¸ ObservaÃ§Ãµes

- Crie o **Ã­ndice Pinecone** chamado `rag-demo` antes de rodar.
- As APIs da Cohere, Pinecone e Groq podem ter limitaÃ§Ãµes ou custos.

---

## ğŸ¤ Contribuindo

Curtiu o projeto? Tem ideia pra melhorar? Manda um PR, issue, ou sÃ³ diz um oi ğŸ‘‹

---

## ğŸ“„ LicenÃ§a

MIT â€” use livremente, sÃ³ lembre de dar os crÃ©ditos ğŸ˜Š

